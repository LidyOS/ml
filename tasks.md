## Задача 1

**Условие:**  
Покажите, что в задаче бинарной классификации с метками  
- либо в множестве Y = {–1, 1} с логистической функцией потерь  
  $$L(M)= \ln\big(1 + e^{-M}\big),\quad M = y f(x),$$  
- либо с метками Y = {0, 1} при оптимизации стандартного log loss  
  $$L(y, f(x)) = -\, y \ln(p) - (1-y)\ln(1-p),\quad p=\sigma(f(x))=\frac{1}{1+e^{-f(x)}},$$  
приводят к одной оптимизационной задаче.

**Решение:**  
1. Пусть сначала используется классическая логистическая регрессия с метками из {0, 1}. Тогда функция потерь для примера выглядит так:
   - Если $y=1$:  
     $$L(1, f(x)) = -\ln\big(p\big) = \ln\Big(1+e^{-f(x)}\Big).$$
   - Если $y=0$:  
     $$L(0, f(x)) = -\ln\big(1-p\big) = -\ln\left(\frac{e^{-f(x)}}{1+e^{-f(x)}}\right) = f(x) + \ln\Big(1+e^{-f(x)}\Big).$$

2. Чтобы привести оба случая к единому виду, можно перейти к меткам в $\{-1,1\}$ через преобразование  
   $$y' = 2y - 1,$$
   тогда при $y'=1$ соответствует $y=1$ и при $y'=-1$ – $y=0$.

3. Определим отступ как  
   $$M=y' f(x).$$  
   Тогда для $y'=1$ логистическая потеря становится
   $$L = \ln\big(1+e^{-f(x)}\big) = \ln\big(1 + e^{-M}\big),$$  
   а для $y'=-1$ (так как $M=-f(x)$) получаем
   $$L = \ln\big(1+e^{f(x)}\big) = \ln\big(1+e^{-(-f(x))}\big) = \ln\big(1+e^{-M}\big).$$

Таким образом, независимо от того, какая кодировка меток используется, оптимизационная задача сводится к минимизации логистической потери  
$$
L(M)= \ln\big(1+e^{-M}\big)
$$  
при условии, что $M=y f(x)$ либо $M = y' f(x)$ с преобразованием $y' = 2y-1$. То есть обе постановки эквивалентны (с точностью до альтернативного определения меток).

---

## Задача 2

**Условие:**  
Покажите, как log loss связан с правдоподобием выборки.

**Решение:**  
Рассмотрим задачу бинарной классификации с метками $y_i \in \{0,1\}$. Пусть модель предсказывает вероятность $p_i = P(y_i=1|x_i)$. Тогда предсказанное распределение для одного наблюдения записывается как:
$$
P(y_i|x_i) = p_i^{y_i} (1-p_i)^{1-y_i}.
$$

Правдоподобие (likelihood) всей выборки (при $n$ независимых наблюдениях) равно:
$$
L = \prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}.
$$

Чтобы сделать задачу удобной с вычислительной точки зрения, берут логарифм и меняют знак, получая **отрицательное логарифмическое правдоподобие**:
$$
-\ln L = -\sum_{i=1}^n \left[ y_i \ln(p_i) + (1-y_i)\ln(1-p_i) \right],
$$
что после деления на $n$ даёт выражение log loss:
$$
\text{Log Loss} = -\frac{1}{n}\sum_{i=1}^n \left[ y_i \ln(p_i) + (1-y_i)\ln(1-p_i) \right].
$$

Таким образом, минимизация log loss эквивалентна максимизации правдоподобия выборки.

---

## Задача 3

**Условие:**  
Покажите с помощью теоремы Куна–Таккера (KKT) эквивалентность регуляризации как ограничения (неравенства) и как штрафного слагаемого в оптимизационной задаче.

**Решение:**  
Рассмотрим общую постановку задачи оптимизации с ограничением:
$$
\min_{w} \; f(w) \quad \text{при условии} \quad R(w) \le \tau.
$$
Можно сформировать Lagrange-функцию:
$$
L(w,\lambda) = f(w) + \lambda (R(w) - \tau), \quad \lambda \ge 0.
$$

Согласно условиям Куна–Таккера, при оптимальности должны выполняться следующие условия:
1. **Примемость решения:** $R(w^*) \le \tau$.
2. **Дополнительное условие:** $\lambda^* (R(w^*)-\tau)=0$ (комплементарная жесткость).
3. **Стационарность:** $ \nabla f(w^*) + \lambda^* \nabla R(w^*) = 0.$

При наличии оптимального решения существует $\lambda^* \ge 0$, такое что решение задачи с ограничением $R(w)\le \tau$ совпадает с решением безусловной задачи:
$$
\min_{w}\; f(w) + \lambda^* R(w)
$$
(при этом константа $\lambda^*$ зависит от строгости ограничения).

Таким образом, регуляризацию можно либо задать явно как ограничение $R(w) \le \tau$, либо включить в целевую функцию в виде штрафа $\lambda R(w)$ с правильным выбором $\lambda$. Это и есть эквивалентность постановки через ограничение и через штрафное слагаемое, демонстрируемая КKT-условиями.

---

## Задача 4

**Условие:**  
Выведите шаг градиентного бустинга для задачи классификации (подсказка: используйте в качестве потерь log loss).

**Решение:**  
Рассмотрим задачу бинарной классификации с метками $y \in \{0,1\}$ и функцию потерь log loss:
$$
L(y, f(x)) = -\left[ y\ln(p) + (1-y)\ln(1-p) \right],
$$
где $p = \sigma(f(x)) = \frac{1}{1+e^{-f(x)}}$.

Для удобства найдём производную логарифмической потери по предсказанному значению $f(x)$. Известно, что  
$$
\frac{d}{df} \ln\Big(1+e^{-f}\Big) = -\frac{e^{-f}}{1+ e^{-f}} = -\big[1-\sigma(f)\big].
$$
При этом, стандартный вывод показывает, что:
$$
\frac{\partial L(y, f(x))}{\partial f(x)} = \sigma(f(x)) - y,
$$
то есть отрицательный градиент (то, что нужно аппроксимировать следующим базовым алгоритмом) равен:
$$
-\frac{\partial L(y, f(x))}{\partial f(x)} = y - \sigma(f(x)) = y - p.
$$

**Интерпретация для градиентного бустинга:**  
На каждом шаге алгоритма градиентного бустинга мы вычисляем для каждого объекта $x_i$ остаток (так называемый «negative gradient»):
$$
r_i = y_i - \sigma\big(f(x_i)\big).
$$

Затем обучается базовый регрессор (например, небольшое решающее дерево) на этих остатках. Если базовый алгоритм выдаёт прогноз $h(x)$, то обновление ансамбля имеет вид:
$$
f_{\text{new}}(x) = f_{\text{old}}(x) + \gamma\, h(x),
$$
где $\gamma$ – шаг (learning rate), а $h(x)$ аппроксимирует величину $y - \sigma(f(x))$.

Таким образом, шаг градиентного бустинга для задачи классификации с log loss состоит в подгонке нового базового алгоритма на отрицательном градиенте ошибки, равном $y - \sigma(f(x))$.

## Задача 5

**Условие:**  
Покажите, что логарифмирование таргета (цели) позволяет свести оптимизацию MAE к оптимизации MAPE. Каковы границы применимости такого подхода? Можете ли вы предложить более широко применимый подход с весами объектов из обучающей выборки? Получится ли применить его же для оптимизации SMAPE?

**Решение:**  

1. **Связь MAE и MAPE через логарифмирование.**  
   Пусть y > 0 – истинное значение, и рассмотрим логарифм преобразования:  
   $$z = \ln y,\quad \hat z = \ln \hat{y}. $$
   Тогда для малых отклонений разложим по Тейлору:
   $$\ln \hat{y} \approx \ln y + \frac{\hat{y}-y}{y}. $$
   Отсюда
   $$|\ln \hat{y} - \ln y| \approx \left|\frac{\hat{y}-y}{y}\right|. $$
   Таким образом, минимизация среднего абсолютного отклонения по логарифмам (MAE в лог-пространстве) приблизительно равносильна минимизации средней процентной ошибки (MAPE) в исходном масштабе:
   $$\frac{1}{n}\sum_{i=1}^{n} \left| \ln \hat{y}_i - \ln y_i \right| \approx \frac{1}{n}\sum_{i=1}^{n} \left|\frac{\hat{y}_i-y_i}{y_i}\right|. $$

2. **Границы применимости.**  
   Данное приближение основывается на линейном разложении ln(·) и справедливо, когда относительные ошибки невелики, а y не близки к нулю. Если y имеет небольшой масштаб или сильно варьируется, линейное приближение может оказаться неточным.

3. **Более общий подход с весами.**  
   Чтобы учитывать относительный вклад каждого примера, можно оптимизировать взвешенную MAE:
   $$\text{WMAE} = \frac{1}{n}\sum_{i=1}^n w_i \, \left| \hat{y}_i-y_i \right|, $$
   выбрав веса, например, равные $w_i = \frac{1}{|y_i|}$. При таком выборе
   $$\text{WMAE} = \frac{1}{n}\sum_{i=1}^n \frac{|\hat{y}_i-y_i|}{|y_i|} $$
   получается точное определение MAPE. Аналогичные идеи можно применить для оптимизации SMAPE, выбрав веса, учитывающие величину $ |y_i|+|\hat{y}_i| $ в знаменателе, хотя тут формулы могут быть несколько изменены для симметричности.

---

## Задача 6

**Условие:**  
Покажите на примере константного прогноза, что оптимизация MSE приводит к оценке среднего, оптимизация MAE – к оценке медианы, а оптимизация log loss – к оценке вероятности.

**Решение:**  

Пусть для выборки $\{y_i\}_{i=1}^n$ мы рассматриваем константный прогноз $c$, то есть $\hat{y}_i = c$ для всех $i$.

1. **MSE.**  
   Минимизируем функцию:
   $$
   \text{MSE}(c) = \frac{1}{n}\sum_{i=1}^n (y_i - c)^2.
   $$
   Для нахождения минимума приравниваем производную к нулю:
   $$
   \frac{d}{dc}\text{MSE}(c) = -\frac{2}{n}\sum_{i=1}^n (y_i - c) = 0
   $$
   $$\Rightarrow c = \frac{1}{n}\sum_{i=1}^n y_i, $$
   то есть оптимальное $c$ равно среднему (mean).

2. **MAE.**  
   Минимизируем:
   $$
   \text{MAE}(c) = \frac{1}{n}\sum_{i=1}^n |y_i - c|.
   $$
   Известно, что минимум абсолютной ошибки достигается в медиане выборки. То есть оптимальное $c$ – медиана.

3. **Log loss (в задаче классификации).**  
   Рассмотрим бинарную классификацию с метками $y_i \in \{0,1\}$. Пусть константный прогноз – вероятность $p$ для класса 1. Тогда log loss:
   $$
   L(p) = -\frac{1}{n}\sum_{i=1}^n \Big[y_i\ln p + (1-y_i)\ln(1-p)\Big].
   $$
   Минимизация этой функции при фиксированной выборке приводит к
   $$
   p^* = \frac{1}{n}\sum_{i=1}^n y_i,
   $$
   то есть оптимальное значение равно доле объектов класса 1, что и есть оценка вероятности.

---

## Задача 7

**Условие:**  
Precision модели равен 0.8. Чему может равняться минимальное и максимальное значение F1-меры? А Accuracy?

**Решение:**  

Для бинарной классификации с метками, пусть:
- Precision $= \frac{TP}{TP+FP} = 0.8$.

При условии, что F1-мера определяется как:
$$
F1 = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}},
$$
и recall может варьироваться от 0 до 1.

- **Минимальное значение F1:**  
  Если recall стремится к 0, то $F1 \to 0$.

- **Максимальное значение F1:**  
  Максимум F1 достигается при recall $= 1$:
  $$
  F1_{\max} = \frac{2 \cdot 0.8 \cdot 1}{0.8 + 1} = \frac{1.6}{1.8} \approx 0.8889.
  $$

**От Accuracy (точность)** зависит не только от precision, но и от соотношения классов и recall. Без дополнительной информации (например, о балансе классов) точные границы определить сложно.  
При сбалансированных классах, если recall также высок, Accuracy может быть близкой к F1, а в худшем случае (при очень низком recall) Accuracy может опускаться до уровня, близкого к доле доминирующего класса. Таким образом, точные границы Accuracy зависят от контекста; можно лишь сказать, что Accuracy всегда не ниже максимальной из precision и recall в идеальных условиях, но без дополнительных данных нельзя дать однозначный числовой интервал лишь исходя из фиксированного precision = 0.8.

---

## Задача 8

**Условие:**  
Как ошибка классификации в листе дерева при случайном ответе с вероятностью, пропорциональной доле класса в листе, связана с критерием Gini?

**Решение:**  

Пусть в узле (листе) класса 1 – доля $p$, а класса 0 – доля $1-p$. Случайный классификатор, делающий прогноз согласно этим вероятностям, имеет вероятность ошибки равную вероятности того, что случайный выбор класса окажется неверным.  
Если случайный прогноз выбирается с вероятностями, равными долям, то ожидаемый процент ошибок:
- При выборе классов: вероятность ошибиться равна минимальной доле, то есть $ \min(p, 1-p)$.

С другой стороны, **Gini impurity** определяется как:
$$
Gini = 1 - \big(p^2 + (1-p)^2\big) = 2p(1-p).
$$

Заметим, что если $p \le 0.5$, то $\min(p, 1-p)=p$ и $2p(1-p) \ge p$ (так как $1-p \ge 0.5$). При $p=0.5$, Gini $= 0.5$ и ошибка случайного выбора $=0.5$. Если $p$ близко к 0 или 1, обе величины стремятся к 0. Таким образом, хотя величины не равны, они обе являются функциями неопределённости в листе. Более того, различным функциям impurity (например, misclassification error $1-\max(p,1-p)$) можно установить монотонную зависимость от $p$ и они служат для оценки однородности листа.  
Иными словами, ожидаемая ошибка случайного классификатора (где прогноз равен долям классов) является альтернативной мерой неоднородности, которая при некоторых условиях ведёт к тому же решению, что и оптимизация impurity по критерию Gini.

---

## Задача 9

**Условие:**  
Покажите, что при бэггинге M регрессоров, ошибки которых являются независимыми одинаково распределёнными случайными величинами, математическое ожидание квадрата ошибки усреднённого регрессора в M раз меньше математического ожидания квадрата ошибки каждого базового алгоритма.

**Решение:**  

Пусть для каждого базового регрессора прогноз $\hat{y}_i^{(m)}$ имеет ошибку $e_i^{(m)} = y_i - \hat{y}_i^{(m)}$ с матожиданием нуля и дисперсией $\sigma^2$, и ошибки между регрессорами независимы. Усреднённый прогноз:
$$
\hat{y}_i = \frac{1}{M} \sum_{m=1}^M \hat{y}_i^{(m)},
$$
имеет ошибку:
$$
e_i = y_i - \hat{y}_i = \frac{1}{M}\sum_{m=1}^M e_i^{(m)}.
$$
Так как $e_i^{(m)}$ независимы, дисперсия суммы равна сумме дисперсий, а дисперсия среднего:
$$
\operatorname{Var}(e_i) = \frac{1}{M^2} \cdot M \sigma^2 = \frac{\sigma^2}{M}.
$$
Так как среднеквадратичная ошибка (MSE) равна дисперсии при нулевом смещении, то
$$
\text{MSE}_{\text{bagged}} = \frac{\sigma^2}{M},
$$
то есть MSE уменьшается ровно в M раз по сравнению с каждым базовым регрессором.

---

## Задача 10

**Условие:**  
Докажите, что в бэггинге при стремлении размера исходной выборки к бесконечности доля её объектов, попадающих в бутстрепированную выборку, будет стремиться к $1-e^{-1}$.

**Решение:**  

При бутстреп-выборке размер исходной выборки равен $N$. При формировании бутстреп-выборки из $N$ объектов объект выбирается с возвращением. Вероятность того, что определённый объект ни разу не выбран, равна:
$$
\left(1 - \frac{1}{N}\right)^N.
$$
При $N \to \infty$ предел:
$$
\lim_{N\to\infty} \left(1 - \frac{1}{N}\right)^N = e^{-1} \approx 0.3679.
$$
Таким образом, доля объектов, попадающих **хотя бы один раз**, равна:
$$
1 - e^{-1} \approx 1 - 0.3679 = 0.6321.
$$

То есть при больших $N$ бутстрепированная выборка покрывает около 63.2% исходной выборки.

